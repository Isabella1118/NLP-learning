1. What is the "cache" used for in our implementation of forward propagation and backward propagation?

We use it to pass variables computed during forward propagation to the corresponding backward propagation step. 
It contains useful values for backward propagation to compute derivatives.

Explanation: Correct, the "cache" records values from the forward propagation units and sends it to the backward propagation units 
because it is needed to compute the chain rule derivatives.

2. Among the following, which ones are "hyperparameters"?

size of the hidden layers n^[l].
number of iterations.
number of layers L in hte neural network.
learning rate alpha.

3. Which of the following statements is true?
The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.

4. Vectorization allows you to compute forward propagation in an L-layer neural network without an explicit for-loop (or any 
other explicit iterative loop) over the layers l=1,2,...,L. True/False?

False.

Explanation: Forward propagation propagates the input through the layers, although for shallow networks we may just 
write all the lines (a^[2] = g^[2](z^[2]), z^[2] = W^[2]a^[1] + b^[2],...) in a deeper network, we cannot avoid a for loop
iterating over the layers: (a^[l] = g^[l](z^[l]), z^[l] = W^[l]a^[l-1]+b^[l],...)

5. Assume we store the values for n^[l] in an array called layers, as follows: layer_dims = [nx, 4,3,2,1]. 
So layer 1 has four hidden units, layer 2 has 3 hidden units and so on. Which of the following for-loops will 
allow you to initialize the parameters for the model?

for(i in range(1, len(layer_dims))):
  parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01
  parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01
  
7. During forward propagation, in the forward function for a layer ll you need to know what is the activation function 
in a layer (Sigmoid, tanh, ReLU, etc.). During backpropagation, the corresponding backward function also needs to know 
what is the activation function for layer ll, since the gradient depends on it. True/False?

True.

Explanation: Yes, as you've seen in the week 3 each activation has a different derivative. Thus, during backpropagation 
you need to know which activation was used in the forward propagation to be able to compute the correct derivative.

8. There are certain functions with the following properties:

(i) To compute the function using a shallow network circuit, you will need a large network (where we measure size by the 
number of logic gates in the network), but (ii) To compute it using a deep network circuit, you need only an exponentially 
smaller network. True/False?

True.

10. Whereas the previous question used a specific network, in the general case what is the dimension of W^{[l]}, 
the weight matrix associated with layer l?

W^[l] has shape (n^[l], n^[l-1])

